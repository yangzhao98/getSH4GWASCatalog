library(roxygen2)
library(devtools)
use_mit_license()
use_package(this.path,type="Imports")
use_package("this.path",type="Imports")
load_all()
test()
use_template()
use_test()
check()
load_all()
build()
check()
load_al
load_all()
document()
check()
build()
load_all()
use_vignette()
use_vignette(name="getSH4GWASCatalog")
document()
check()
use_github()
use_git()
load_all()
document()
check()
git_sitrep()
use_git_config(user.name="yangzhao98",user.email="yangz98@connect.hku.hk")
git_sitrep()
use_git
use_git()
use_readme_md()
load_all()
build()
check()
devtools::install_github()
use_github()
gh_token_help()
create_git
create_github_token()
use_github()
gh_token_help()
gitcreds::gitcreds_set()
use_github()
use_git()
use_github()
rlang::last_trace()
git_default_branch_rediscover()
load_all()
document()
check()
load_all()
document()
check()
build()
accessionNumber <- "GCST90002000"
## Setup the fixed ftp website
.url <- "https://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics"
## Generate the access number range
genRange <- floor(as.numeric(gsub("GCST","",accessionNumber))/1000)
genRange
2000 %/% 1000
is.integr(2000 %/% 1000)
is.intger(2000 %/% 1000)
is.integer(2000 %/% 1000)
?is.integer()
is.integer((2000 %/% 1000))
is.integer((2001 %/% 1000))
is.integer((2001 %% 1000))
(2001 %% 1000)%%1==0
(2001 %/% 1000)%%1==0
(2001 %% 1000)%%1==0
all(2001 %% 1000)%%1==0
## Generate the access number range
x <- as.numeric(gsub("GCST","",accessionNumber))/1000
all(x%%1==0)
x
all((x+0.1)%%1==0)
if (all(x%%1==0)) { genRange <- x-1 }
#' @title Generate a shell script for downloading summary-level data from GWAS Catalog
#' @param accessionNumberList The accession number or a vector of accession numbers
#' @export
getSH4GWASCatalog <- function(accessionNumberList) {
## Generate the url for downloading summary statistics from GWAS Catalog
getUrlFromGWASCatalog <- function(accessionNumber) {
## Setup the fixed ftp website
.url <- "https://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics"
## Generate the access number range
x <- as.numeric(gsub("GCST","",accessionNumber))/1000
if (all(x%%1==0)) { genRange <- x-1 } else {genRange <- floor(x)}
.url. <- paste("GCST",
c(genRange*1000+1,(genRange+1)*1000),
collapse="-",sep="")
## Generate the dataset
url. <- paste(accessionNumber,"/",
paste(accessionNumber,"_buildGRCh37.tsv.gz",sep=""),
sep="")
paste(.url,"/",.url.,"/",url.,sep="")
}
urls <- unlist(lapply(accessionNumberList,
FUN=function(i) {
aref <- getUrlFromGWASCatalog(i)
paste("wget ",aref," &",sep="")
} ))
## Generate shell script for downloading summary statistics in parallel
## This step is very useful when downloading multiple datasets
aHead <- "!#/bin/bash"
aEnd1 <- "wait &"
aEnd2 <- "echo All datasets are downloaded"
datUrls <- rbind(data.frame(x=aHead),data.frame(x=urls),
data.frame(x=aEnd1),data.frame(x=aEnd2))
return(datUrls)
}
getSH4GWASCatalog("GCST90001001")
getSH4GWASCatalog("GCST90001000")
getSH4GWASCatalog("GCST90002000")
getSH4GWASCatalog("GCST90002001")
rm(list=ls())
library(roxygen2)
library(testthat)
library(this.path)
load_all()
library(pkgload)
load_all()
build()
library(devtools)
build()
document()
check()
use_vignette()
# 设置要爬取的网址
url <- "https://pheweb.jp/downloads"
# 发送HTTP请求并获取网页内容
response <- GET(url)
content <- content(response, "text")
library(rvest)
library(httr)
# 设置要爬取的网址
url <- "https://pheweb.jp/downloads"
# 发送HTTP请求并获取网页内容
response <- GET(url)
content <- content(response, "text")
# 使用rvest解析HTML内容
page <- read_html(content)
# 使用CSS选择器提取数据，这里假设你要提取所有链接
links <- page %>% html_nodes("a") %>% html_attr("href")
# 打印提取的链接
print(links)
rm(list=ls())
library(devtools)
library(roxygen2)
library(this.path)
use_package("rvest",type="Imports")
use_package("httr",type="Imports")
load_all()
document()
use_vignette()
use_package_doc()
build()
build_manual()
load_all()
check()
use_package("magrittr",type="magrittr")
use_package("magrittr",type="imports")
load_all()
document()
check()
?html_attr
url <- "https://pheweb.jp/downloads"
links <- rvest::html_attr(
rvest::html_nodes(
rvest::read_html(httr::content(httr::GET(url),"text")),name="a"),name="href")
links <- rvest::html_attr(
rvest::html_nodes(
rvest::read_html(httr::content(httr::GET(url),"text")),"a"),"href")
links
rm(list=ls())
load_all()
document()
build()
check()
load_all()
document()
check()
build()
build_manual()
load_all()
document()
update_packages()
rm(list=ls())
load_all()
document()
check()
install()
accessionNumber <- ""GCST90001002""
accessionNumber <- "GCST90001002"
## Setup the fixed ftp website
.url <- "https://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics"
## Generate the access number range
x <- as.numeric(gsub("GCST","",accessionNumber))/1000
if (all(x%%1==0)) { genRange <- x-1 } else {genRange <- floor(x)}
.url. <- paste("GCST",
c(genRange*1000+1,(genRange+1)*1000),
collapse="-",sep="")
## Generate the dataset
url <- paste(.url,"/",.url.,"/",accessionNumber,"/harmonised/",sep="")
url
#' @title Get all links from a url
#'
#' @param url The url of a website
#'
#' @export
getHttpsFromURL <- function(url) {
links <- rvest::html_attr(
rvest::html_nodes(
rvest::read_html(httr::content(httr::GET(url),"text")),"a"),"href")
return(links)
}
links <- getHttpsFromURL(url)
links
url
## Generate the dataset
url <- paste(.url,"/",.url.,"/",accessionNumber,"/harmonised",sep="")
links <- getHttpsFromURL(url)
links
links <- httr::GET(url)
links
library(devtools)
library(roxygen2)
library(this.path)
use_package("Rcurl",type="Imports")
links <- RCurl::getURLContent(url)
links <- RCurl::getURL(url)
links
url
## Generate the dataset
url <- paste(.url,"/",.url.,"/",accessionNumber,"/harmonised/",sep="")
links <- RCurl::getURL(url)
links
urls
url
accessionNumber <- "GCST90002000"
## Setup the fixed ftp website
.url <- "https://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics"
## Generate the access number range
x <- as.numeric(gsub("GCST","",accessionNumber))/1000
if (all(x%%1==0)) { genRange <- x-1 } else {genRange <- floor(x)}
.url. <- paste("GCST",
c(genRange*1000+1,(genRange+1)*1000),
collapse="-",sep="")
## Generate the dataset
url <- paste(.url,"/",.url.,"/",accessionNumber,"/harmonised/",sep="")
links <- RCurl::getURL(url)
links
links <- RCurl::getURLContent(url)
links
links <- strsplit(RCurl::getURL(url), "\n")[[1]]
links
links <- getHttpsFromURL(url)
links
links <- grep(".h.tsv.gz",links,value=TRUE)
links
paste(url,links,sep="")
rm(list=ls())
load_all()
document()
check()
load_all()
document()
check()
install()
build()
library(getSH4GWASCatalog)
rm(list=ls())
load_all()
library(devtools)
library(roxygen2)
library(this.path)
load_all()
document()
check()
load_all
load_all()
document()
check()
load_all()
document()
check()
load_all()
document()
check()
install()
library(devtools)
library(roxygen2)
library(this.path)
load_all()
document()
check()
library(getSH4GWASCatalog)
accnum <- "GCST003116"
getHarmonizedUrlsFromGWASCatalog(accnum)
getHarmonizedUrlsFromGWASCatalog(as.list(accnum))
accessionNumberLists <- accnum
urls <- unlist(
lapply(accessionNumberLists,
FUN=function(i) {
## Setup the fixed ftp website
.url <- "https://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics"
## Generate the access number range
x <- as.numeric(gsub("GCST","",i))/1000
if (all(x%%1==0)) { genRange <- x-1 } else {genRange <- floor(x)}
.url. <- paste("GCST",
c(genRange*1000+1,(genRange+1)*1000),
collapse="-",sep="")
## Generate the dataset
url <- paste(.url,"/",.url.,"/",i,"/harmonised/",sep="")
links <- getHttpsFromURL(url)
links <- grep(".h.tsv.gz",links,value=TRUE)
return(paste(url,links,sep=""))
}))
urls
url<-urls
links <- getHttpsFromURL(url)
links
url
httr::GET(url)
httr::content(httr::GET(url),"a")
httr::content(httr::GET(url),"text")
rvest::read_html(httr::content(httr::GET(url),"text"),"a")
url <- "https://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/GCST003001-GCST004000/GCST003116/harmonised/"
rm(list=ls())
